{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f443d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\chira\\anaconda3\\lib\\site-packages (4.37.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dbe463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: transformers in c:\\users\\chira\\anaconda3\\lib\\site-packages (4.37.1)\n",
      "Requirement already satisfied: keras in c:\\users\\chira\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Obtaining dependency information for keras from https://files.pythonhosted.org/packages/a3/31/982a0c8da5e06b8e915e09e7cae7f7815eecfef7e9e16fd733b105aa09ab/keras-3.0.4-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.0.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (2.0.0)\n",
      "Requirement already satisfied: rich in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (13.7.0)\n",
      "Requirement already satisfied: namex in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (3.7.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (0.1.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Using cached keras-3.0.4-py3-none-any.whl (1.0 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "Successfully installed keras-3.0.4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b1f8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\chira\\anaconda3\\lib\\site-packages (4.37.1)\n",
      "Requirement already satisfied: keras in c:\\users\\chira\\anaconda3\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\chira\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (2.0.0)\n",
      "Requirement already satisfied: rich in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (13.7.0)\n",
      "Requirement already satisfied: namex in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (3.7.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\chira\\anaconda3\\lib\\site-packages (from keras) (0.1.8)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Collecting keras\n",
      "  Obtaining dependency information for keras from https://files.pythonhosted.org/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.0.4\n",
      "    Uninstalling keras-3.0.4:\n",
      "      Successfully uninstalled keras-3.0.4\n",
      "Successfully installed keras-2.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8566bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chira\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bce4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=230, min_length=30, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fb5c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chira\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chira\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef3b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in c:\\users\\chira\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (6.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (0.20.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (0.4.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chira\\anaconda3\\lib\\site-packages (from diffusers) (9.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.2->diffusers) (2023.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.2->diffusers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.2->diffusers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.2->diffusers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.2->diffusers) (23.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->diffusers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->diffusers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.20.2->diffusers) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install diffusers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa224a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arabert in c:\\users\\chira\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyArabic in c:\\users\\chira\\anaconda3\\lib\\site-packages (from arabert) (0.6.15)\n",
      "Requirement already satisfied: farasapy in c:\\users\\chira\\anaconda3\\lib\\site-packages (from arabert) (0.0.14)\n",
      "Requirement already satisfied: emoji==1.4.2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from arabert) (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\anaconda3\\lib\\site-packages (from farasapy->arabert) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chira\\anaconda3\\lib\\site-packages (from farasapy->arabert) (4.65.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from PyArabic->arabert) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\anaconda3\\lib\\site-packages (from tqdm->farasapy->arabert) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arabert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be33286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\chira\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مواجهات عنيفة بين الجيش اللبناني ومحتجين في طرابلس\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name = \"malmarjeh/t5-arabic-text-summarization\"\n",
    "preprocessor = ArabertPreprocessor(model_name=\"\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "text2text_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"شهدت مدينة طرابلس، مساء أمس الأربعاء، احتجاجات شعبية وأعمال شغب لليوم الثالث على التوالي، وذلك بسبب تردي الوضع المعيشي والاقتصادي. واندلعت مواجهات عنيفة وعمليات كر وفر ما بين الجيش اللبناني والمحتجين استمرت لساعات، إثر محاولة فتح الطرقات المقطوعة، ما أدى إلى إصابة العشرات من الطرفين.\"\n",
    "text = preprocessor.preprocess(text)\n",
    "\n",
    "result = text2text_pipeline(\n",
    "    text,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=3,\n",
    "    repetition_penalty=3.0,\n",
    "    max_length=200,\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=3\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7b33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το Food Pass - το τρίτο επίδομα που εξετάζει η κυβέρνηση έπειτα από τις επιδοτήσεις σε καύσιμα και ηλεκτρικό ρεύμα, θα γίνει κοντά στις γιορτές των Χριστουγέννων για έναν μήνα για ευάλωτες ομάδες. Σύμφωνα με πληροφορίες της Πανελλήνια Ομοσπονδίας Καταστηματαρχών Κρεοπωλών (ΠΟΚΚ) Οικονομικών διαμαρτυρήθηκαν σχετικά στο μέτρο εξαργύρωσης του Covid-19. Όπως προκύπτει: Υπενθυμίζεται πώς οι καταν\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"kriton/greek-text-summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "article = \"\"\" Στη ΔΕΘ πρόκειται να ανακοινωθεί και με τον πλέον επίσημο τρόπο το Food Pass - το τρίτο επίδομα που εξετάσει η κυβέρνηση έπειτα από τις επιδοτήσεις σε καύσιμα και ηλεκτρικό ρεύμα.\n",
    "Η επιταγή ακρίβειας για τρόφιμα θα δοθεί εφάπαξ σε οικογένειες, που πληρούν συγκεκριμένα κριτήρια, με στόχο να στηριχτούν όσοι πλήττονται περισσότερο από την αύξηση του πληθωρισμού και την ακρίβεια, όπως:\n",
    "χαμηλοσυνταξιούχοι, άνεργοι και ευάλωτες κοινωνικές ομάδες.\n",
    "Σύμφωνα με τις διαθέσιμες πληροφορίες, η πληρωμή θα γίνει κοντά στις γιορτές των Χριστουγέννων, όταν - σύμφωνα με εκτιμήσεις - θα έχει βαθύνει η ενεργειακή κρίση και θα έχουν αυξηθεί ακόμη παραπάνω οι ανάγκες των νοικοκυριών.\n",
    "Ουσιαστικά, με τον τρόπο αυτό, η κυβέρνηση θα επιδοτήσει τις αγορές του σούπερ μάρκετ για έναν μήνα για ευάλωτες ομάδες.\n",
    "Αντιδράσεις από τους κρεοπώλες \n",
    "Υπενθυμίζεται πως την Πέμπτη η Πανελλήνια Ομοσπονδία Καταστηματαρχών Κρεοπωλών (ΠΟΚΚ) Οικονομικών - με επιστολή της προς τα συναρμόδια υπουργεία - διαμαρτυρήθηκε σχετικά με τη χορήγηση του Food Pass στα σούπερ μάρκετ.\n",
    "Στην επιστολή της ομοσπονδίας, την οποία κοινοποίησε και στην Κεντρική Ένωση Επιμελητηρίων και στη ΓΣΕΒΕΕ, σημειώνεται ότι εάν το μέτρο εξαργύρωσης του επιδόματος τροφίμων αφορά μόνο στις αλυσίδες τροφίμων τότε απαξιώνεται ο κλάδος των κρεοπωλών και \"εντείνεται ο αθέμιτος ανταγωνισμός, εφόσον κατευθύνεται ο καταναλωτής σε συγκεκριμένες επαγγελματικές κατηγορίες\".\n",
    "Στο πλαίσιο αυτό, οι κρεοπώλες ζητούν να διευθυνθεί η λίστα των επαγγελματιών όπου ο καταναλωτής θα μπορεί να εξαργυρώσει το εν λόγω βοήθημα.\n",
    "\"\"\"\n",
    "\n",
    "def generate_summary(article):\n",
    "    inputs = tokenizer(\n",
    "        'summarize: ' + article, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=512, \n",
    "        min_length=130, \n",
    "        length_penalty=3.0, \n",
    "        num_beams=8, \n",
    "        early_stopping=True,\n",
    "        repetition_penalty=3.0,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_summary(article))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"Context: سعي; Char: س; Pos: 0\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pain/text2svg_summarization-2-epochs-28-step-367000\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"pain/text2svg_summarization-2-epochs-28-step-367000\")\n",
    "outputs = model.generate(inputs, max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "tok_out = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "tok_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43878a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def compute_rouge(reference_summary, generated_summary):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "    return scores\n",
    "\n",
    "def compute_jaccard_similarity(reference_summary, generated_summary):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([reference_summary, generated_summary])\n",
    "    similarity = (vectors * vectors.T).A[0, 1]\n",
    "    return similarity\n",
    "\n",
    "# Load a pre-trained sentence embedding model\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Reference summaries\n",
    "reference_summary = \"\"\"\n",
    "Hugging Face is a leading company in NLP. The company was founded in 2016.\n",
    "The Transformers library is a significant contribution to NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Actual data for Model 1\n",
    "article_model_1 = \"\"\"\n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "... (rest of the article)\n",
    "\"\"\"\n",
    "\n",
    "# Model 1\n",
    "summarizer_model_1 = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "generated_summary_model_1 = summarizer_model_1(article_model_1, max_length=230, min_length=30, do_sample=False)\n",
    "\n",
    "# Compute ROUGE scores for Model 1\n",
    "rouge_scores_model_1 = compute_rouge(reference_summary, generated_summary_model_1[0]['summary_text'])\n",
    "print(\"Model 1 ROUGE scores:\", rouge_scores_model_1)\n",
    "\n",
    "# Compute fluency scores for Model 1\n",
    "source_embedding_model_1 = embedder.encode(article_model_1, convert_to_tensor=True)\n",
    "generated_summary_embedding_model_1 = embedder.encode(generated_summary_model_1[0]['summary_text'], convert_to_tensor=True)\n",
    "fluency_score_model_1 = util.pytorch_cos_sim(source_embedding_model_1, generated_summary_embedding_model_1).item()\n",
    "print(\"Model 1 Fluency Score:\", fluency_score_model_1)\n",
    "\n",
    "# Compute content coverage scores for Model 1\n",
    "jaccard_similarity_model_1 = compute_jaccard_similarity(reference_summary, generated_summary_model_1[0]['summary_text'])\n",
    "print(\"Model 1 Jaccard Similarity (Content Coverage):\", jaccard_similarity_model_1)\n",
    "\n",
    "# Model 3\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "# Actual data for Model 3\n",
    "text_model_3 = \"شهدت مدينة طرابلس، مساء أمس الأربعاء، احتجاجات شعبية وأعمال شغب لليوم الثالث على التوالي، وذلك بسبب تردي الوضع المعيشي والاقتصادي. واندلعت مواجهات عنيفة وعمليات كر وفر ما بين الجيش اللبناني والمحتجين استمرت لساعات، إثر محاولة فتح الطرقات المقطوعة، ما أدى إلى إصابة العشرات من الطرفين.\"\n",
    "\n",
    "# Model 3\n",
    "model_name_model_3 = \"malmarjeh/t5-arabic-text-summarization\"\n",
    "tokenizer_model_3 = AutoTokenizer.from_pretrained(model_name_model_3)\n",
    "model_model_3 = AutoModelForSeq2SeqLM.from_pretrained(model_name_model_3)\n",
    "text2text_pipeline_model_3 = pipeline(\"text2text-generation\", model=model_model_3, tokenizer=tokenizer_model_3)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessor = ArabertPreprocessor(model_name=model_name_model_3)  # Add this line to define the preprocessor\n",
    "text_model_3_preprocessed = preprocessor.preprocess(text_model_3)\n",
    "\n",
    "# Generate summary\n",
    "result_model_3 = text2text_pipeline_model_3(\n",
    "    text_model_3_preprocessed,\n",
    "    pad_token_id=tokenizer_model_3.eos_token_id,\n",
    "    num_beams=3,\n",
    "    repetition_penalty=3.0,\n",
    "    max_length=200,\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=3\n",
    ")[0]['generated_text']\n",
    "\n",
    "# Compute ROUGE scores for Model 3\n",
    "rouge_scores_model_3 = compute_rouge(reference_summary, result_model_3)\n",
    "print(\"Model 3 ROUGE scores:\", rouge_scores_model_3)\n",
    "\n",
    "# Compute fluency scores for Model 3\n",
    "source_embedding_model_3 = embedder.encode(text_model_3_preprocessed, convert_to_tensor=True)\n",
    "generated_summary_embedding_model_3 = embedder.encode(result_model_3, convert_to_tensor=True)\n",
    "fluency_score_model_3 = util.pytorch_cos_sim(source_embedding_model_3, generated_summary_embedding_model_3).item()\n",
    "print(\"Model 3 Fluency Score:\", fluency_score_model_3)\n",
    "\n",
    "# Compute content coverage scores for Model 3\n",
    "jaccard_similarity_model_3 = compute_jaccard_similarity(reference_summary, result_model_3)\n",
    "print(\"Model 3 Jaccard Similarity (Content Coverage):\", jaccard_similarity_model_3)\n",
    "\n",
    "\n",
    "# Model 4\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Actual data for Model 4\n",
    "text_model_4 = \"Context: سعي; Char: س; Pos: 0\"\n",
    "reference_summary_model_4 = \"Actual reference summary for Model 4\"\n",
    "\n",
    "# Model 4\n",
    "tokenizer_model_4 = AutoTokenizer.from_pretrained(\"pain/text2svg_summarization-2-epochs-28-step-367000\")\n",
    "model_model_4 = AutoModelForSeq2SeqLM.from_pretrained(\"pain/text2svg_summarization-2-epochs-28-step-367000\")\n",
    "\n",
    "# Replace placeholder with actual data\n",
    "inputs_model_4 = tokenizer_model_4(text_model_4, return_tensors=\"pt\").input_ids\n",
    "outputs_model_4 = model_model_4.generate(inputs_model_4, max_new_tokens=1024, do_sample=False)\n",
    "generated_summary_model_4 = tokenizer_model_4.decode(outputs_model_4[0], skip_special_tokens=True)\n",
    "\n",
    "# Compute ROUGE scores for Model 4\n",
    "rouge_scores_model_4 = compute_rouge(reference_summary_model_4, generated_summary_model_4)\n",
    "print(\"Model 4 ROUGE scores:\", rouge_scores_model_4)\n",
    "\n",
    "# Compute fluency scores for Model 4\n",
    "source_embedding_model_4 = embedder.encode(text_model_4, convert_to_tensor=True)\n",
    "generated_summary_embedding_model_4 = embedder.encode(generated_summary_model_4, convert_to_tensor=True)\n",
    "fluency_score_model_4 = util.pytorch_cos_sim(source_embedding_model_4, generated_summary_embedding_model_4).item()\n",
    "print(\"Model 4 Fluency Score:\", fluency_score_model_4)\n",
    "\n",
    "# Compute content coverage scores for Model 4\n",
    "jaccard_similarity_model_4 = compute_jaccard_similarity(reference_summary_model_4, generated_summary_model_4)\n",
    "print(\"Model 4 Jaccard Similarity (Content Coverage):\", jaccard_similarity_model_4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
